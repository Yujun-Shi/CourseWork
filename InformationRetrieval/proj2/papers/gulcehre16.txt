Noisy Activation Functions

Caglar Gulcehre†(cid:93)
Marcin Moczulski†(cid:5)
Misha Denil†(cid:5)
Yoshua Bengio†(cid:93)
†(cid:93)University of Montreal
†(cid:5)University of Oxford

Abstract

Common nonlinear activation functions used in
neural networks can cause training difﬁculties
due to the saturation behavior of the activation
function, which may hide dependencies that
are not visible to vanilla-SGD (using ﬁrst order
gradients only). Gating mechanisms that use
softly saturating activation functions to emulate
the discrete switching of digital logic circuits are
good examples of this. We propose to exploit the
injection of appropriate noise so that the gradients
may ﬂow easily, even if the noiseless application of
the activation function would yield zero gradients.
Large noise will dominate the noise-free gradient
and allow stochastic gradient descent to explore
more. By adding noise only to the problematic
parts of the activation function, we allow the
optimization procedure to explore the boundary
between the degenerate (saturating) and the
well-behaved parts of the activation function. We
also establish connections to simulated annealing,
when the amount of noise is annealed down,
making it easier to optimize hard objective func-
tions. We ﬁnd experimentally that replacing such
saturating activation functions by noisy variants
helps optimization in many contexts, yielding
state-of-the-art or competitive results on different
datasets and task, especially when training seems
to be the most difﬁcult, e.g., when curriculum
learning is necessary to obtain good results.

1. Introduction
The introduction of the piecewise-linear activation functions
such as ReLU and Maxout (Goodfellow et al., 2013) units
had a profound effect on deep learning, and was a major
catalyst in allowing the training of much deeper networks. It
is thanks to ReLU that for the ﬁrst time it was shown (Glorot

Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48.
Copyright 2016 by the author(s).

GULCEHRC@IRO.UMONTREAL.CA
MARCIN.MOCZULSKI@STCATZ.OX.AC.UK
MISHA.DENIL@GMAIL.COM
BENGIOY@IRO.UMONTREAL.CA

et al., 2011) that deep purely supervised networks can be
trained, whereas using tanh nonlinearity only allowed to
train shallow networks. A plausible hypothesis about the
recent surge of interest on these piecewise-linear activation
functions (Glorot et al., 2011), is due to the fact that they
are easier to optimize with SGD and backpropagation than
smooth activation functions, such as sigmoid and tanh. The
recent successes of piecewise linear functions is particularly
evident in computer vision, where the ReLU has become the
default choice in convolutional networks.
We propose a new technique to train neural networks with
activation functions which strongly saturate when their input
is large. This is mainly achieved by injecting noise to the
activation function in its saturated regime and learning the
level of noise. Using this approach, we have found that it
was possible to train neural networks with much wider family
of activation functions than previously. Adding noise to the
activation function has been considered for ReLU units and
was explored in (Bengio et al., 2013; Nair & Hinton, 2010) for
feed-forward networks and Boltzmann machines to encourage
units to explore more and make the optimization easier.
More recently there has been a resurgence of interest in more
elaborated “gated” architectures such as LSTMs (Hochreiter
& Schmidhuber, 1997) and GRUs (Cho et al., 2014), but also
encompassing neural attention mechanisms that have been
used in the NTM (Graves et al., 2014), Memory Networks
(Weston et al., 2014), automatic image captioning (Xu et al.,
2015b), video caption generation (Yao et al., 2015) and wide
areas of applications (LeCun et al., 2015). A common thread
running through these works is the use of soft-saturating
non-linearities, such as the sigmoid or softmax, to emulate
the hard decisions of digital logic circuits. In spite of its
success, there are two key problems with this approach.

1. Since the non-linearities still saturate there are problems
with vanishing gradient information ﬂowing through the
gates; and

2. since the non-linearities only softly saturate they do not

allow one to take hard decisions.

Although gates often operate in the soft-saturated regime
(Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al.,

Noisy Activation Functions

2015) the architecture prevents them from being fully open or
closed. We follow a novel approach to address both of these
problems. Our method addresses the second problem through
the use of hard-saturating nonlinearities, which allow gates
to make perfectly on or off decisions when they saturate.
Since the gates are able to be completely open or closed, no
information is lost through the leakiness of the soft-gating
architecture.
By introducing hard-saturating nonlinearities, we have
exacerbated the problem of gradient ﬂow, since gradients
in the saturated regime are now precisely zero instead of
being negligible. However, by introducing noise into the
activation function which can grow based on the magnitude
of saturation, we encourage random exploration. Our work
builds up on the existing literature on the noise injection
methods to the piecewise-linear activation functions (Bengio
et al., 2013; Nair & Hinton, 2010; Xu et al., 2015a).
At test time the noise in the activation functions can be
replaced with its expectation. As our experiments show,
the resulting deterministic networks outperform their
soft-saturating counterparts on a wide variety of tasks, and
allow to reach state-of-the-art performance by simple drop-in
replacement of the nonlinearities in existing training code.
The technique that we propose, addresses the difﬁculty of
optimization and having hard-activations at the test time for
gating units and we propose a way of performing simulated
annealing for neural networks.
Hannun et al. (2014); Le et al. (2015) used ReLU activation
functions with simple RNNs. In this paper, we successfully
show that, it is possible to use piecewise-linear activation
functions with gated recurrent networks such as LSTM and
GRU’s.

2. Saturating Activation Functions
Deﬁnition 2.1. (Activation Function). An activation func-
tion is a function h : R → R that is differentiable almost
everywhere.
Deﬁnition 2.2. (Saturation). An activation function h(x)
with derivative h(cid:48)
(x) is said to right (resp. left) saturate if
its limit as x → ∞ (resp. x → −∞) is zero. An activation
function is said to saturate (without qualiﬁcation) if it both
left and right saturates.
Most common activation functions used in recurrent networks
(for example, tanh and sigmoid) are saturating. In particular
they are soft saturating, meaning that they achieve saturation
only in the limit.
Deﬁnition 2.3. (Hard and Soft Saturation). Let c be a
constant such that x > c implies h(cid:48)
(x) = 0 and left hard
saturates when x < c implies h(cid:48)
(x) = 0, ∀x. We say that
h(·) hard saturates (without qualiﬁcation) if it both left and
right hard saturates. An activation function that saturates but
achieves zero gradient only in the limit is said to soft saturate.

We can construct hard saturating versions of soft saturating

activation functions by taking a ﬁrst-order Taylor expansion
about zero and clipping the results to an appropriate range.
For example, expanding tanh and sigmoid around 0, with
x ≈ 0, we obtain linearized functions ut and us of tanh and
sigmoid respectively:

sigmoid(x) ≈ us(x) = 0.25x + 0.5

tanh(x) ≈ ut(x) = x.

Clipping the linear approximations result to,

hard-sigmoid(x) = max(min(us(x), 1), 0)
hard-tanh(x) = max(min(ut(x), 1), − 1).

(1)
(2)

(3)
(4)

The motivation behind this construction is to introduce linear
behavior around zero to allow gradients to ﬂow easily when
the unit is not saturated, while providing a crisp decision in
the saturated regime.

Figure 1. The plot for derivatives of different activation functions.

The ability of the hard-sigmoid and hard-tanh to make crisp
decisions comes at the cost of exactly 0 gradients in the
saturated regime. This can cause difﬁculties during training:
a small but not inﬁnitesimal change of the pre-activation
(before the nonlinearity) may help to reduce the objective
function, but this will not be reﬂected in the gradient.
In the rest of the document we will use h(x) to refer to
a generic activation function and use u(x) to denote its
linearization based on the ﬁrst-order Taylor expansion about
zero. hard-sigmoid saturates when x (cid:54) −2 or x (cid:62) 2 and
hard-tanh saturates when x (cid:54) −1 or x (cid:62) 1. We denote the
threshold by xt. Absolute values of the threshold are xt = 2
for hard-sigmoid and xt = 1 for the hard-tanh.
The highly non-smooth gradient descent trajectory may bring
the parameters into a state that pushes the activations of a
unit towards the 0 gradient regime for a particular example,
from where it may become difﬁcult to escape and the unit
may get stuck in the 0 gradient regime.
When units saturate and gradients vanish, an algorithm may

−4−2024−0.20.00.20.40.60.81.01.2derivativeofhard-tanhderivativeoftanhderivativeofsigmoidderivativeofhardsigmoidNoisy Activation Functions

require many training examples and a lot of computation to
recover.

3. Annealing with Noisy Activation Functions
Consider a noisy activation function φ(x, ξ) in which we
have injected iid noise ξ, to replace a saturating nonlinearity
such as the hard-sigmoid and hard-tanh introduced in
the previous section. In the next section we describe the
proposed noisy activation function which has been used for
our experiments, but here we want to consider a larger family
of such noisy activation functions, when we use a variant
of stochastic gradient descent (SGD) for training.
Let ξ have variance σ2 and mean 0. We want to characterize
what happens as we gradually anneal the noise, going from
large noise levels (σ → ∞) to no noise at all (σ → 0).
Furthermore, we will assume that φ is such that when the
noise level becomes large, so does its derivative with respect
to x:

lim|ξ|→∞|

∂φ(x, ξ)

∂x

| → ∞.

(5)

Figure 2. An example of a one-dimensional, non-convex objective
function where a simple gradient descent will behave poorly. With
large noise |ξ| → ∞, SGD can escape from saddle points and bad
local-minima as a result of exploration. As we anneal the noise level
|ξ| → 0, SGD will eventually converge to one of the local-minima
x∗.

In the 0 noise limit, we recover a deterministic nonlinearity,
φ(x, 0), which in our experiments is piecewise linear and
allows us to capture the kind of complex function we want to
learn. As illustrated in Figure 2, in the large noise limit, large
gradients are obtained because backpropagating through φ
gives rise to large derivatives. Hence, the noise drowns the
signal: the example-wise gradient on parameters is much
larger than it would have been with σ = 0. SGD therefore
just sees noise and can move around anywhere in parameter
space without “seeing” any trend.
Annealing is also related to the signal to noise ratio where
SN R can be deﬁned as the ratio of the variance of noise
σsignal and σnoise, SN R = σsignal
. If SN R → 0, the model
σnoise
will do pure random exploration. As we anneal SN R will
increase, and when σnoise converges to 0, the only source of

exploration during the training will come from the noise of
Monte Carlo estimates of stochastic gradients.
This is precisely what we need for methods such as simulated
annealing (Kirkpatrick et al., 1983) and continuation meth-
ods (Allgower & Georg, 1980) to be helpful, in the context of
the optimization of difﬁcult non-convex objectives. With high
noise, SGD is free to explore all parts of space. As the noise
level is decreased, it will prefer some regions where the signal
is strong enough to be “visible” by SGD: given a ﬁnite number
of SGD steps, the noise is not averaged out, and the variance
continues to dominate. Then as the noise level is reduced SGD
spends more time in “globally better” regions of parameter
space. As it approaches to zero we are ﬁne-tuning the solution
and converging near a minimum of the noise-free objective
function. A related approach of adding noise to gradients and
annealing the noise was investigated in (Neelakantan et al.,
2015) as well. Ge et al. (2015) showed that SGD with an-
nealed noise will globally converge to a local-minima for non-
convex objective functions in polynomial number of iterations.
Recently, Mobahi (2016) propose an optimization method
that applies Gaussian smoothing on the loss function such that
annealing weight noise is a Monte Carlo estimator of that.

4. Adding Noise when the Unit Saturate
A novel
idea behind the proposed noisy activation is
that the amount of noise added to the nonlinearity is
proportional to the magnitude of saturation of the
nonlinearity. For hard-sigmoid(x) and hard-tanh(x),
due to our parametrization of the noise, that translates into the
fact that the noise is only added when the hard-nonlinearity
saturates. This is different from previous proposals such as
the noisy rectiﬁer from Bengio (2013) where noise is added
just before a rectiﬁer (ReLU) unit, independently of whether
the input is in the linear regime or in the saturating regime
of the nonlinearity.
The motivation is to keep the training signal clean when the
unit is in the non-saturating (typically linear) regime and
provide some noisy signal when the unit is in the saturating
regime.
h(x) refer to hard saturation activation function such as the
hard-sigmoid and hard-tanh introduced in Sec. 2, we consider
noisy activation functions of the following form:

φ(x, ξ) = h(x) + s

(6)

and s = µ + σξ. Here ξ is an iid random variable drawn
from some generating distribution, and the parameters µ
and σ (discussed below) are used to generate a location scale
family from ξ.
Intuitively when the unit saturates we pin its output to the
threshold value t and add noise. The exact behavior of the
method depends on the type of noise ξ and the choice of
µ and σ, which we can pick as functions of x in order to
let some gradients be propagated even when we are in the
saturating regime.

|ξ|→∞|ξ|→0x*Noisy Activation Functions

A desirable property we would like φ to approximately
satisfy is that, in expectation, it is equal to the hard-saturating
activation function, i.e.

Eξ∼N (0,1)[φ(x, ξ)] ≈ h(x)

(7)

If the ξ distribution has zero mean then this property can
be satisﬁed by setting µ = 0, but for biased noise it will be
necessary to make other choices for µ. In practice, we used
slightly biased φ with good results.
Intuitively we would like to add more noise when x is far
into the saturated regime, since a large change in parameters
would be required desaturate h. Conversely, when x is close
to the saturation threshold a small change in parameters
would be sufﬁcient for it to escape. To that end we make use
of the difference between the original activation function h
and its linearization u

∆ = h(x) − u(x)

(8)

when choosing the scale of the noise.
See Eqs.1 for
deﬁnitions of u for the hard-sigmoid and hard-tanh
respectively. The quantity ∆ is zero in the unsaturated
regime, and when h saturates it grows proportionally to the
distance between |x| and the saturation threshold xt. We also
refer |∆| as the magnitude of the saturation.
We experimented with different ways of scaling σ with
∆, and empirically found that the following formulation
performs better:

σ(x) = c (g(p∆) − 0.5)2
g(x) = sigmoid(x).

(9)

In Equation 9 a free scalar parameter p is learned during the
course of training. By changing p, the model is able to adjust
the magnitude of the noise and that also effects the sign of
the gradient as well. The hyper-parameter c changes the scale
of the standard deviation of the noise.

4.1. Derivatives in the Saturated Regime
In the simplest case of our method we draw ξ from an unbi-
ased distribution, such as a standard normal. In this case we
choose µ = 0 to satisfy Equation 7 and therefore we will have,

φ(x, ξ) = h(x) + σ(x)ξ

Due to our parameterization of σ(x), when |x| ≤ xt our
stochastic activation function behaves exactly as the linear
function u(x), leading to familiar territory. Because ∆ will
be 0. Let us for the moment restrict our attention to the case
when |x| > xt and h saturates. In this case the derivative
of h(x) is precisely zero, however, if we condition on the
sample ξ we have

(cid:48)

φ

(x, ξ) =

∂
∂x

φ(x, ξ) = σ

(cid:48)

(x)ξ

(10)

Figure 3. A simple depiction of adding Gaussian noise on the
linearized activation function, which brings the average back to
the hard-saturating nonlinearity h(x), in bold. Its linearization is
u(x) and the noisy activation is φ. The difference h(x) − u(x) is
∆ which is a vector indicates the discrepancy between the linearized
function and the actual function that the noise is being added to
h(x). Note that, ∆ will be zero, at the non-saturating parts of the
function where u(x) and h(u) matches perfectly.

which is non-zero almost surely.
In the non-saturated regime, where φ(cid:48)(x, ξ) = h(cid:48)
(x) the op-
timization can exploit the linear structure of h near the origin
in order to tune its output. In the saturated regime the random-
ness in ξ drives exploration, and gradients still ﬂow back to
x since the scale of the noise still depends on x. To reiterate,
we get gradient information at every point in spite of the
saturation of h, and the variance of the gradient information
in the saturated regime depends on the variance of σ(cid:48)(x)ξ.

4.2. Pushing Activations towards Linear Regime
An unsatisfying aspect of the formulation with unbiased noise
is that, depending on the value of ξ occasionally the gradient
of φ will point the wrong way. This can cause a backwards
message that would push x in a direction that would worsen
the objective function on average over ξ.
Intuitively we
would prefer these messages to “push back” the saturated
unit towards a non-saturated state where the gradient of h(x)
can be used safely.
A simple way to achieve this is to make sure that the noise
ξ is always positive and adjust its sign manually to match
the sign of x. In particular we could set

d(x) = − sgn(x) sgn(1 − α)
s =µ(x) + d(x)σ(x)|ξ|.

where ξ and σ are as before and sgn is the sign function, such
that sgn(x) is 1 if x is greater than or equal to 0 otherwise it is
−1. We also use the absolute value of ξ in the reparametriza-
tion of the noise, such that the noise is being sampled from a
half-Normal distribution. We ignored the sign of ξ, such that
the direction that the noise pushes the activations are deter-
mined by d(x) and it will point towards h(x). Matching the

-1E[ϕ(x)] = h(x)xxtu(x)+1ϕ(x)～N(h(x), σ2)ΔNoisy Activation Functions

sign of the noise to the sign of x would ensure that we avoid
the sign cancellation between the noise and the gradient mes-
sage from backpropagation. sgn(1−α) is required to push the
activations towards h(x) when the bias from α is introduced.
In practice we use a hyperparameter α that inﬂuences the
mean of the added term, such that α near 1 approximately
satisﬁes the above condition, as seen in Fig. 4. We can
rewrite the noisy term s in a way that the noise can either
be added to the linearized function or h(x). The relationship
between ∆, u(x) and h(x) is visualized Figure 4.1 can be
expressed as in Eqn 11.
We have experimented with different
types of noise.
Empirically, in terms of performance we found, half-normal
and normal noise to be better. In Eqn 11, we provide the
formulation for the activation function where  = |ξ| if the
noise is sampled from half-normal distribution,  = ξ if the
noise is sampled from normal distribution.

φ(x, ξ) = u(x) + α∆ + d(x)σ(x)

(11)

By using Eqn 11, we arrive at the noisy activations, which
we used in our experiments.

φ(x, ξ) = αh(x) + (1 − α)u(x) + d(x)σ(x)

(12)

As can be seen in Eqn 12, there are three paths that gradients
can ﬂow through the neural network, the linear path (u(x)),
nonlinear path (h(x)) and the stochastic path (σ(x)). The
ﬂow of gradients through these different pathways across
different layers makes the optimization of our activation
function easier.
At test time, we used the expectation of Eqn 12 in order to
get deterministic units,
Eξ[φ(x, ξ)] = αh(x)+(1−α)u(x)+d(x)σ(x)Eξ[] (13)
If  = ξ, then Eξ[] is 0. Otherwise if  = |ξ|, then Eξ[]
is

(cid:113) 2

π .

Algorithm 1 Noisy Activations with Half-Normal Noise for
Hard-Saturating Functions
1: ∆ ← h(x) − u(x)
2: d(x) ← − sgn(x) sgn(1 − α)
3: σ(x) ← c (g(p∆) − 0.5)2
4: ξ ∼ N (0, 1)
5: φ(x, ξ) ← αh(x) + (1 − α)u(x) + (d(x)σ(x)|ξ|)
To illustrate the effect of α and noisy activation of the
hard-tanh, We provide plots of our stochastic activation
functions in Fig 4.

5. Adding Noise to Input of the Function
Adding noise with ﬁxed standard deviation to the input of the
activation function has been investigated for ReLU activation

Figure 4. Stochastic behavior of the proposed noisy activation
function with different α values and with noise sampled from the
Normal distribution, approximating the hard-tanh nonlinearity
(in bold green).

functions (Nair & Hinton, 2010; Bengio et al., 2013).

φ(x, ξ) = h(x + σξ) and ξ ∼ N (0, 1).

(14)

In Eqn 14, we provide a parametrization of the noisy
activation function. σ can be either learned as in Eqn 9 or
ﬁxed as a hyperparameter.
The condition in Eqn 5 is satisﬁed only when σ is learned.
Experimentally we found small values of σ to work better.
When σ is ﬁxed and small, as x gets larger and further away
from the threshold xt, noise will less likely be able to push the
activations back to the linear regime. We also investigated the
effect of injecting input noise when the activations saturate:

φ(x, ξ) = h(x + 1|x|≥|xt|(σξ)) and ξ ∼ N (0, 1).

(15)

6. Experimental Results
In our experiments, we used noise only during training: at
test time we replaced the noise variable with its expected
value. We performed our experiments with just a drop-in re-
placement of the activation functions in existing experimental
setups, without changing the previously set hyper-parameters.
Hence it is plausible one could obtain better results by
performing a careful hyper-parameter tuning for the models
with noisy activation functions. In all our experiments, we
initialized p uniform randomly from the range [−1, 1].
We provide experimental results using noisy activations
with normal (NAN), half-normal noise (NAH), normal
noise at the input of the function (NANI), normal noise
at the input of the function with learned σ (NANIL)
and normal noise injected to the input of the function
when the unit saturates (NANIS). Codes for different
types of noisy activation functions can be found at
https://github.com/caglar/noisy_units.

−4−2024−1.5−1.0−0.50.00.51.01.5SimulatingTanhActivationFunctionwithHalf-NormalNoisewrtDiﬀerentAlphasalpha=0.9alpha=1.0alpha=1.1alpha=1.2HardTanh