Auxiliary Deep Generative Models

Lars Maaløe1
Casper Kaae Sønderby2
Søren Kaae Sønderby2
Ole Winther1,2
1Department of Applied Mathematics and Computer Science, Technical University of Denmark
2Bioinformatics Centre, Department of Biology, University of Copenhagen

LARSMA@DTU.DK
CASPERKAAE@GMAIL.COM
SKAAESONDERBY@GMAIL.COM
OLWI@DTU.DK

Abstract

Deep generative models parameterized by neu-
ral networks have recently achieved state-of-
the-art performance in unsupervised and semi-
supervised learning. We extend deep genera-
tive models with auxiliary variables which im-
proves the variational approximation. The aux-
iliary variables leave the generative model un-
changed but make the variational distribution
more expressive. Inspired by the structure of the
auxiliary variable we also propose a model with
two stochastic layers and skip connections. Our
ﬁndings suggest that more expressive and prop-
erly speciﬁed deep generative models converge
faster with better results. We show state-of-the-
art performance within semi-supervised learning
on MNIST, SVHN and NORB datasets.

1. Introduction
Stochastic backpropagation, deep neural networks and ap-
proximate Bayesian inference have made deep generative
models practical for large scale problems (Kingma, 2013;
Rezende et al., 2014), but typically they assume a mean
ﬁeld latent distribution where all latent variables are in-
dependent. This assumption might result in models that
are incapable of capturing all dependencies in the data. In
this paper we show that deep generative models with more
expressive variational distributions are easier to optimize
and have better performance. We increase the ﬂexibility of
the model by introducing auxiliary variables (Agakov and
Barber, 2004) allowing for more complex latent distribu-
tions. We demonstrate the beneﬁts of the increased ﬂexibil-
ity by achieving state-of-the-art performance in the semi-
supervised setting for the MNIST (LeCun et al., 1998),

SVHN (Netzer et al., 2011) and NORB (LeCun et al., 2004)
datasets.
Recently there have been signiﬁcant improvements within
the semi-supervised classiﬁcation tasks. Kingma et al.
(2014) introduced a deep generative model for semi-
supervised learning by modeling the joint distribution over
data and labels. This model is difﬁcult to train end-to-end
with more than one layer of stochastic latent variables, but
coupled with a pretrained feature extractor it performs well.
Lately the Ladder network (Rasmus et al., 2015; Valpola,
2014) and virtual adversarial training (VAT) (Miyato et al.,
2015) have improved the performance further with end-to-
end training.
In this paper we train deep generative models with mul-
tiple stochastic layers. The Auxiliary Deep Generative
Models (ADGM) utilize an extra set of auxiliary latent vari-
ables to increase the ﬂexibility of the variational distribu-
tion (cf. Sec. 2.2). We also introduce a slight change to
the ADGM, a 2-layered stochastic model with skip con-
nections, the Skip Deep Generative Model (SDGM) (cf.
Sec. 2.4). Both models are trainable end-to-end and offer
state-of-the-art performance when compared to other semi-
supervised methods. In the paper we ﬁrst introduce toy data
to demonstrate that:

(i) The auxiliary variable models can ﬁt complex la-
tent distributions and thereby improve the variational
lower bound (cf. Sec. 4.1 and 4.3).

(ii) By ﬁtting a complex half-moon dataset using only six
labeled data points the ADGM utilizes the data mani-
fold for semi-supervised classiﬁcation (cf. Sec. 4.2).

For the benchmark datasets we show (cf. Sec. 4.4):

(iii) State-of-the-art results on several semi-supervised

classiﬁcation tasks.

Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume
48. Copyright 2016 by the author(s).

(iv) That multi-layered deep generative models for semi-
supervised learning are trainable end-to-end without
pre-training or feature engineering.

Auxiliary Deep Generative Models

2. Auxiliary deep generative models
Recently Kingma (2013); Rezende et al. (2014) have cou-
pled the approach of variational inference with deep learn-
ing giving rise to powerful probabilistic models constructed
by an inference neural network q(z|x) and a generative
neural network p(x|z). This approach can be perceived as
a variational equivalent to the deep auto-encoder, in which
q(z|x) acts as the encoder and p(x|z) the decoder. How-
ever, the difference is that these models ensures efﬁcient
inference over continuous distributions in the latent space
z with automatic relevance determination and regulariza-
tion due to the KL-divergence. Furthermore, the gradients
of the variational upper bound are easily deﬁned by back-
propagation through the network(s). To keep the computa-
tional requirements low the variational distribution q(z|x)
is usually chosen to be a diagonal Gaussian, limiting the
expressive power of the inference model.
In this paper we propose a variational auxiliary vari-
able approach (Agakov and Barber, 2004) to improve
the variational distribution: The generative model is ex-
tended with variables a to p(x, z, a) such that the original
model is invariant to marginalization over a: p(x, z, a) =
p(a|x, z)p(x, z).
In the variational distribution, on the
other hand, a is used such that marginal q(z|x) =

(cid:82) q(z|a, x)p(a|x)da is a general non-Gaussian distribution.

This hierarchical speciﬁcation allows the latent variables to
be correlated through a, while maintaining the computa-
tional efﬁciency of fully factorized models (cf. Fig. 1). In
Sec. 4.1 we demonstrate the expressive power of the infer-
ence model by ﬁtting a complex multimodal distribution.

The variational auto-encoder (VAE) has recently been in-
troduced as a powerful method for unsupervised learning.
Here a latent variable generative model pθ(x|z) for data x
is parameterized by a deep neural network with parameters
θ. The parameters are inferred by maximizing the varia-
i UVAE(xi) with

tional lower bound of the likelihood −(cid:80)

(cid:90)

z

(cid:20)

log pθ(x) = log

p(x, z)dz

pθ(x|z)pθ(z)

qφ(z|x)

≥ Eqφ(z|x)
log
≡ −UVAE(x) .

(cid:21)

(1)

The inference model qφ(z|x) is parameterized by a second
deep neural network. The inference and generative param-
eters, θ and φ, are jointly trained by optimizing Eq. 1 with
stochastic gradient ascent, where we use the reparameter-
ization trick for backpropagation through the Gaussian la-
tent variables (Kingma, 2013; Rezende et al., 2014).

2.1. Variational auto-encoder

log pθ(x) = log

p(x, a, z)dadz

(a) Generative model P .

(b) Inference model Q.

Figure 1. Probabilistic graphical model of the ADGM for semi-
supervised learning. The incoming joint connections to each vari-
able are deep neural networks with parameters θ and φ.
2.2. Auxiliary variables

We propose to extend the variational distribution with aux-
iliary variables a: q(a, z|x) = q(z|a, x)q(a|x) such that the
marginal distribution q(z|x) can ﬁt more complicated pos-
teriors p(z|x). In order to have an unchanged generative
model, p(x|z), it is required that the joint mode p(x, z, a)
gives back the original p(x, z) under marginalization over
a, thus p(x, z, a) = p(a|x, z)p(x, z). Auxiliary variables
are used in the EM algorithm and Gibbs sampling and
have previously been considered for variational learning
by Agakov and Barber (2004). Concurrent with this work
Ranganath et al. (2015) have proposed to make the param-
eters of the variational distribution stochastic, which leads
to a similar model.
It is important to note that in order
not to fall back to the original VAE model one has to re-
quire p(a|x, z) (cid:54)= p(a), see Agakov and Barber (2004) and
App. A. The auxiliary VAE lower bound becomes

(cid:90)

(cid:90)

a

z

(cid:20)

≥ Eqφ(a,z|x)
log
≡ −UAVAE(x) .

(cid:21)

(2)

pθ(a|z, x)pθ(x|z)p(z)
qφ(a|x)qφ(z|a, x)

with pθ(a|z, x) and qφ(a|x) diagonal Gaussian distribu-
tions parameterized by deep neural networks.

2.3. Semi-supervised learning

The main focus of this paper is to use the auxiliary ap-
proach to build semi-supervised models that learn clas-
siﬁers from labeled and unlabeled data.
To encom-
pass the class information we introduce an extra la-
tent variable y. The generative model P is deﬁned as
p(y)p(z)pθ(a|z, y, x)pθ(x|y, z) (cf. Fig. 1a):

p(z) = N (z|0, I),
p(y) = Cat(y|π),

pθ(a|z, y, x) = f (a; z, y, x, θ),
pθ(x|z, y) = f (x; z, y, θ) ,

(3)
(4)
(5)
(6)

yzaxyzaxAuxiliary Deep Generative Models

where a, y, z are the auxiliary variable, class label, and la-
tent features, respectively. Cat(·) is a multinomial distribu-
tion, where y is treated as a latent variable for the unlabeled
data points. In this study we only experimented with cate-
gorical labels, however the method applies to other distri-
butions for the latent variable y. f (x; z, y, θ) is iid categori-
cal or Gaussian for discrete and continuous observations x.
pθ(·) are deep neural networks with parameters θ. The in-
ference model is deﬁned as qφ(a|x)qφ(z|a, y, x)qφ(y|a, x)
(cf. Fig. 1b):

φ(x))),

qφ(a|x) = N (a|µφ(x), diag(σ2

qφ(y|a, x) = Cat(y|πφ(a, x)),

(7)
(8)
qφ(z|a, y, x) = N (z|µφ(a, y, x), diag(σ2
φ(a, y, x))) . (9)
In order to model Gaussian distributions pθ(a|z, y, x),
pθ(x|z, y), qφ(a|x) and qφ(z|a, y, x) we deﬁne two sepa-
rate outputs from the top deterministic layer in each deep
neural network, µφ∨θ(·) and log σ2
φ∨θ(·). From these out-
puts we are able to approximate the expectations E by ap-
plying the reparameterization trick.
The key point of the ADGM is that the auxiliary unit a
introduce a latent feature extractor to the inference model
giving a richer mapping between x and y. We can use the
classiﬁer (9) to compute probabilities for unlabeled data xu
being part of each class and to retrieve a cross-entropy er-
ror estimate on the labeled data xl. This can be used in
cohesion with the variational lower bound to deﬁne a good
objective function in order to train the model end-to-end.

VARIATIONAL LOWER BOUND

We optimize the model by maximizing the lower bound on
the likelihood (cf. App. B for more details). The variational
lower bound on the marginal likelihood for a single labeled
data point is

(cid:90)

(cid:90)

(cid:20)

log pθ(x, y) = log
≥ Eqφ(a,z|x,y)
≡ −L(x, y) ,

log

p(x, y, a, z)dzda

(cid:21)

z

a
pθ(x, y, a, z)
qφ(a, z|x, y)

with qφ(a, z|x, y) = qφ(a|x)qφ(z|a, y, x). For unlabeled
data we further introduce the variational distribution for y,
qφ(y|a, x):

log pθ(x) = log

p(x, y, a, z)dzdyda

(cid:90)

(cid:90)

(cid:90)

(cid:20)

a

log

z

y
pθ(x, y, a, z)
qφ(a, y, z|x)

(cid:21)

≥ Eqφ(a,y,z|x)
≡ −U(x) ,

with qφ(a, y, z|x) = qφ(z|a, y, x)qφ(y|a, x)qφ(a|x).

The classiﬁer (9) appears in −U(xu), but not in −L(xl, yl).
The classiﬁcation accuracy can be improved by introducing
an explicit classiﬁcation loss for labeled data:
Ll(xl, yl) =

(12)
L(xl, yl) + α · log Eqφ(a|xl)[− log qφ(yl|a, xl)] ,
where α is a weight between generative and discriminative
learning. The α parameter is set to β · Nl+Nu
, where β is
a scaling constant, Nl is the number of labeled data points
and Nu is the number of unlabeled data points. The objec-
tive function for labeled and unlabeled data is
U(xu) .

Ll(xl, yl) +

(cid:88)

(cid:88)

J =

(13)

Nl

(xl,yl)

(xu)

2.4. Two stochastic layers with skip connections

On the other hand,

Kingma et al. (2014) proposed a model with two stochas-
tic layers but were unable to make it converge end-
to-end and instead resorted to layer-wise training.
In
our preliminary analysis we also found that this model:
pθ(x|z1)pθ(z1|z2, y)p(z2)p(y) failed to converge when
trained end-to-end.
the auxil-
iary model can be made into a two-layered stochastic
model by simply reversing the arrow between a and
x in Fig. 1a. We would expect that if the auxiliary
model works well in terms of convergence and perfor-
mance then this two-layered model (a is now part of the
generative model): pθ(x|y, a, z)pθ(a|z, y)p(z)p(y) should
work even better because it is a more ﬂexible genera-
tive model. The variational distribution is unchanged:
qφ(z|y, x, a)qφ(y|a, x)qφ(a|x). We call this the Skip Deep
Generative Model (SDGM) and test it alongside the auxil-
iary model in the benchmarks (cf. Sec. 4.4).

3. Experiments
The SDGM and ADGM are each parameterized by 5 neu-
ral networks (NN): (1) auxiliary inference model qφ(a|x),
(2) latent inference model qφ(z|a, y, x), (3) classiﬁcation
model qφ(y|a, x), (4) generative model pθ(a|·), and (5) the
generative model pθ(x|·).
The neural networks consists of M fully connected hidden
layers with hj denoting the output of a layer j = 1, ..., M.
All hidden layers use rectiﬁed linear activation functions.
To compute the approximations of the stochastic variables
we place two independent output layers after hM , µ and
log σ2. In a forward-pass we are propagating the input x
through the neural network by

hM =NN(x)

µ =Linear(hM )
log σ2 =Linear(hM ) ,

(14)
(15)
(16)

(10)

(11)

Auxiliary Deep Generative Models

with Linear denoting a linear activation function. We then
approximate the stochastic variables by applying the repa-
rameterization trick using the µ and log σ2 outputs.
In the unsupervised toy example (cf. Sec. 4.1) we ap-
plied 3 hidden layers with dim(h) = 20, dim(a) = 4
and dim(z) = 2. For the semi-supervised toy example (cf.
Sec. 4.2) we used two hidden layers of dim(h) = 100 and
dim(a, z) = 10.
For all the benchmark experiments (cf. Sec. 4.4) we
parameterized the deep neural networks with two fully
connected hidden layers.
Each pair of hidden layers
was of size dim(h) = 500 or dim(h) = 1000 with
dim(a, z) = 100 or dim(a, z) = 300. The generative
model was p(y)p(z)pθ(a|z, y)pθ(x|z, y) for the ADGM
and the SDGM had the augmented pθ(x|a, z, y). Both have
unchanged inference models (cf. Fig. 1b).
All parameters are initialized using the Glorot and Bengio
(2010) scheme. The expectation over the a and z variables
were performed by Monte Carlo sampling using the repa-
rameterization trick (Kingma, 2013; Rezende et al., 2014)
and the average over y by exact enumeration so
Eqφ(a,y,z|x) [f (a, x, y, z)] ≈
Nsamp(cid:88)
(cid:88)

q(y|ai, x)f (ai, x, y, zyi),

(17)

1

Nsamp

i

y

with ai ∼ q(a|x) and zyi ∼ q(z|a, y, x).
For training, we have used the Adam (Kingma and Ba,
2014) optimization framework with a learning rate of 3e-
4, exponential decay rate for the 1st and 2nd moment at 0.9
and 0.999, respectively. The β constant was between 0.1
and 2 throughout the experiments.
The models are implemented in Python using Theano
(Bastien et al., 2012), Lasagne (Dieleman et al., 2015) and
Parmesan libraries1.
For the MNIST dataset we have combined the training set
of 50000 examples with the validation set of 10000 exam-
ples. The test set remained as is. We used a batch size
of 200 with half of the batch always being the 100 labeled
samples. The labeled data are chosen randomly, but dis-
tributed evenly across classes. To speed up training, we
removed the columns with a standard deviation below 0.1
resulting in an input size of dim(x) = 444. Before each
epoch the normalized MNIST images were binarized by
sampling from a Bernoulli distribution with mean parame-
ter set to the pixel intensities.
For the SVHN dataset we used the vectorized and cropped
training set dim(x) = 3072 with classes from 0 to 9, com-

1Implementation is available in a repository named auxiliary-

deep-generative-models on github.com.

bined with the extra set resulting in 604388 data points.
The test set is of size 26032. We trained on the small
NORB dataset consisting of 24300 training samples and an
equal amount of test samples distributed across 5 classes:
animal, human, plane,
truck, car. We normalized all
NORB images following Miyato et al. (2015) using image
pairs of 32x32 resulting in a vectorized input of dim(x) =
2048. The labeled subsets consisted of 1000 evenly dis-
tributed labeled samples. The batch size for SVHN was
2000 and for NORB 200, where half of the batch was la-
beled samples. To avoid the phenomenon on modeling dis-
cretized values with a real-valued estimation (Uria et al.,
2013), we added uniform noise between 0 and 1 to each
pixel value. We normalized the NORB dataset by 256 and
the SVHN dataset by the standard deviation on each color
channel. Both datasets were assumed Gaussian distributed
for the generative models pθ(x|·).

4. Results
In this section we present two toy examples that shed light
on how the auxiliary variables improve the distribution
ﬁt. Thereafter we investigate the unsupervised generative
log-likelihood performance followed by semi-supervised
classiﬁcation performance on several benchmark datasets.
We demonstrate state-of-the-art performance and show that
adding auxiliary variables increase both classiﬁcation per-
formance and convergence speed (cf. Sec. 3 for details).

4.1. Beyond Gaussian latent distributions
In variational auto-encoders the inference model qφ(z|x) is
parameterized as a fully factorized Gaussian. We demon-
strate that the auxiliary model can ﬁt complicated posterior
distributions for the latent space. To do this we consider the
2D potential model p(z) = exp(U (z))/Z (Rezende and
Mohamed, 2015) that leads to the bound

(cid:20)

log Z ≥ Eqφ(a,z)

log

(cid:21)

exp(U (z))pθ(a|z)

qφ(a)qφ(z|a)

.

(18)

Fig. 2a shows the true posterior and Fig. 2b shows a den-
sity plot of z samples from a ∼ qφ(a) and z ∼ qφ(z|a)
from a trained ADGM. This is similar to the ﬁndings of
Rezende and Mohamed (2015) in which they demonstrate
that by using normalizing ﬂows they can ﬁt complicated
posterior distributions. The most frequent solution found
in optimization is not the one shown, but one where Q ﬁts
only one of the two equivalent modes. The one and two
mode solution will have identical values of the bound so it
is to be expected that the simpler single mode solution will
be easier to infer.

Auxiliary Deep Generative Models

(a)

(b)

Figure 2. (a) True posterior of the prior p(z). (b) The approximation qφ(z|a)qφ(a) of the ADGM. (c) Prediction on the half-moon data
set after 10 epochs with only 3 labeled data points (black) for each class. (d) PCA plot on the 1st and 2nd principal component of the
corresponding auxiliary latent space.

(c)

(d)

4.2. Semi-supervised learning on two half-moons

the ADGM for

To exemplify the power of
semi-
supervised learning we have generated a 2D synthetic
dataset consisting of
two half-moons (top and bot-
tom), where (xtop, ytop) = (cos([0, π]), sin([0, π])) and
(xbottom, ybottom) = (1 − cos([0, π]), 1 − sin([0, π]) − 0.5),
with added Gaussian noise. The training set contains 1e4
samples divided into batches of 100 with 3 labeled data
points in each class and the test set contains 1e4 samples.
A good semi-supervised model will be able to learn the data
manifold for each of the half-moons and use this together
with the limited labeled information to build the classiﬁer.
The ADGM converges close to 0% classiﬁcation error in 10
epochs (cf. Fig. 2c), which is much faster than an equiv-
alent model without the auxiliary variable that converges
in more than 100 epochs. When investigating the auxiliary
variable we see that it ﬁnds a discriminating internal repre-
sentation of the data manifold and thereby aids the classiﬁer
(cf. Fig. 2d).

4.3. Generative log-likelihood performance

We evaluate the generative performance of the unsuper-
vised auxiliary model, AVAE, using the MNIST dataset.
The inference and generative models are deﬁned as

qφ(a, z|x) = qφ(a1|x)qφ(z1|a1, x)

qφ(ai|ai−1, x)qφ(zi|ai, zi−1) ,

L(cid:89)
L−1(cid:89)

i=2

(19)

(20)

pθ(x, a, z) = pθ(x|z1)p(zL)pθ(aL|zL)

pθ(zi|zi+1)pθ(ai|z≥i) .

i=1

where L denotes the number of stochastic layers.
We report the lower bound from Eq. (2) for 5000 impor-
tance weighted samples and use the same training and pa-
rameter settings as in Sønderby et al. (2016) with warm-

up2, batch normalization and 1 Monte Carlo and IW sample
for training.

VAE+NF, L=1 (REZENDE AND MOHAMED, 2015)
IWAE, L=1, IW=1 (BURDA ET AL., 2015)
IWAE, L=1, IW=50 (BURDA ET AL., 2015)
IWAE, L=2, IW=1 (BURDA ET AL., 2015)
IWAE, L=2, IW=50 (BURDA ET AL., 2015)
VAE+VGP, L=2 (TRAN ET AL., 2015)
LVAE, L=5, IW=1 (SØNDERBY ET AL., 2016)
LVAE, L=5, FT, IW=10 (SØNDERBY ET AL., 2016)

AUXILIARY VAE (AVAE), L=1, IW=1
AUXILIARY VAE (AVAE), L=2, IW=1

≤ log p(x)
−85.10
−86.76
−84.78
−85.33
−82.90
−81.90
−82.12
−81.74
−84.59
−82.97

Table 1. Unsupervised test log-likelihood on permutation invari-
ant MNIST for the normalizing ﬂows VAE (VAE+NF), impor-
tance weighted auto-encoder (IWAE), variational Gaussian pro-
cess VAE (VAE+VGP) and Ladder VAE (LVAE) with FT denot-
ing the ﬁnetuning procedure from Sønderby et al. (2016), IW the
importance weighted samples during training, and L the number
of stochastic latent layers z1, .., zL.

We evaluate the negative log-likelihood for the 1 and 2 lay-
ered AVAE. We found that warm-up was crucial for activa-
tion of the auxiliary variables. Table 1 shows log-likelihood
scores for the permutation invariant MNIST dataset. The
methods are not directly comparable, except for the Lad-
der VAE (LVAE) (Sønderby et al., 2016), since the train-
ing is performed differently. However, they give a good
indication on the expressive power of the auxiliary vari-
able model. The AVAE is performing better than the VAE
with normalizing ﬂows (Rezende and Mohamed, 2015) and
the importance weighted auto-encoder with 1 IW sample
(Burda et al., 2015). The results are also comparable to the
Ladder VAE with 5 latent layers (Sønderby et al., 2016)
and variational Gaussian process VAE (Tran et al., 2015).
As shown in Burda et al. (2015) and Sønderby et al. (2016)
increasing the IW samples and annealing the learning rate
will likely increase the log-likelihood.

2Temperature on the KL-divergence going from 0 to 1 within

the ﬁrst 200 epochs of training.

Auxiliary Deep Generative Models

M1+TSVM (KINGMA ET AL., 2014)
M1+M2 (KINGMA ET AL., 2014)
VAT (MIYATO ET AL., 2015)
LADDER NETWORK (RASMUS ET AL., 2015)

AUXILIARY DEEP GENERATIVE MODEL (ADGM)
SKIP DEEP GENERATIVE MODEL (SDGM)

MNIST
100 LABELS
11.82% (±0.25)
3.33% (±0.14)
2.12%
1.06% (±0.37)
0.96% (±0.02)
1.32% (±0.07)

SVHN
1000 LABELS
55.33% (±0.11)
36.02%(±0.10)
24.63%
-

22.86%
16.61% (±0.24)

NORB
1000 LABELS
18.79% (±0.05)
-
9.88%
-
10.06% (±0.05)
9.40% (±0.04)

Table 2. Semi-supervised test error % benchmarks on MNIST, SVHN and NORB for randomly labeled and evenly distributed data
points. The lower section demonstrates the benchmarks of the contribution of this article.

4.4. Semi-supervised benchmarks

MNIST EXPERIMENTS

Table 2 shows the performance of the ADGM and SDGM
on the MNIST dataset. The ADGM’s convergence to
around 2% is fast (around 200 epochs), and from that point
the convergence speed declines and ﬁnally reaching 0.96%
(cf. Fig. 5). The SDGM shows close to similar perfor-
mance and proves more stable by speeding up convergence,
due to its more advanced generative model. We achieved
the best results on MNIST by performing multiple Monte
Carlo samples for a ∼ qφ(a|x) and z ∼ qφ(z|a, y, x).
A good explorative estimate of the models ability to com-
prehend the data manifold, or in other words be as close
to the posterior distribution as possible, is to evaluate the
generative model.
In Fig. 3a we show how the SDGM,
trained on only 100 labeled data points, has learned to sep-
arate style and class information. Fig 3b shows random
samples from the generative model.

(a)

(b)

Figure 3. MNIST analogies. (a) Forward propagating a data point
x (left) through qφ(z|a, x) and generate samples pθ(x|yj, z) for
each class label yj (right). (b) Generating a sample for each class
label from random generated Gaussian noise; hence with no use
of the inference model.

Fig. 4a demonstrate the information contribution from the
stochastic unit ai and zj (subscripts i and j denotes a unit)
in the SDGM as measured by the average over the test set
of the KL-divergence between the variational distribution
and the prior. Units with little information content will be
close to the prior distribution and the KL-divergence term

will thus be close to 0. The number of clearly activated
units in z and a is quite low ∼ 20, but there is a tail of
slightly active units, similar results have been reported by
Burda et al. (2015). It is still evident that we have informa-
tion ﬂowing through both variables though. Fig. 2d and 4b
shows clustering in the auxiliary space for both the ADGM
and SDGM respectively.

(a)

(b)

Figure 4. SDGM trained on 100 labeled MNIST. (a) The KL-
divergence for units in the latent variables a and z calculated by
the difference between the approximated value and its prior. (b)
PCA on the 1st and 2nd principal component of the auxiliary la-
tent space.

Auxiliary Deep Generative Models

5. Discussion
The ADGM and SDGM are powerful deep generative mod-
els with relatively simple neural network architectures.
They are trainable end-to-end and since they follow the
principles of variational inference there are multiple im-
provements to consider for optimizing the models like us-
ing the importance weighted bound or adding more lay-
ers of stochastic variables. Furthermore we have only pro-
posed the models using a Gaussian latent distribution, but
the model can easily be extended to other distributions
(Ranganath et al., 2014; 2015).
One way of approaching the stability issues of the ADGM,
when training on Gaussian input distributions x is to
add a temperature weighting between discriminative and
stochastic learning on the KL-divergence for a and z when
estimating the variational lower bound (Sønderby et al.,
2016). We ﬁnd similar problems for the Gaussian input dis-
tributions in van den Oord et al. (2016), where they restrict
the dataset to ordinal values in order to apply a softmax
function for the output of the generative model p(x|·). This
discretization of data is also a possible solution. Another
potential stabilizer is to add batch normalization (Ioffe and
Szegedy, 2015) that will ensure normalization of each out-
put batch of a fully connected hidden layer.
A downside to the semi-supervised variational framework
is that we are summing over all classes in order to evaluate
the variational bound for unlabeled data. This is a com-
putationally costly operation when the number of classes
grow. In this sense, the Ladder network has an advantage.
A possible extension is to sample y when calculating the
unlabeled lower bound −U(xu), but this may result in gra-
dients with high variance.
The framework is implemented with fully connected layers.
VAEs have proven to work well with convolutional layers
so this could be a promising step to further improve the
performance. Finally, since we expect that the variational
bound found by the auxiliary variable method is quite tight,
it could be of interest to see whether the bound for p(x, y)
may be used for classiﬁcation in the Bayes classiﬁer man-
ner p(y|x) ∝ p(x, y).
6. Conclusion
We have introduced a novel framework for making the vari-
ational distributions used in deep generative models more
expressive. In two toy examples and the benchmarks we in-
vestigated how the framework uses the auxiliary variables
to learn better variational approximations. Finally we have
demonstrated that the framework gives state-of-the-art per-
formance in a number of semi-supervised benchmarks and
is trainable end-to-end.

Figure 5. 100 labeled MNIST classiﬁcation error % evaluated ev-
ery 10 epochs between equally optimized SDGM, ADGM, M2
(Kingma et al., 2014) and an ADGM with a deterministic auxil-
iary variable.

In order to investigate whether the stochasticity of the aux-
iliary variable a or the network depth is essential to the
models performance, we constructed an ADGM with a de-
terministic auxiliary variable. Furthermore we also imple-
mented the M2 model of Kingma et al. (2014) using the
exact same hyperparameters as for learning the ADGM.
Fig. 5 shows how the ADGM outperforms both the M2
model and the ADGM with deterministic auxiliary vari-
ables. We found that the convergence of the M2 model
was highly unstable; the one shown is the best obtained.

SVHN & NORB EXPERIMENTS

From Table 2 we see how the SDGM outperforms VAT
with a relative reduction in error rate of more than 30% on
the SVHN dataset. We also tested the model performance,
when we omitted the SVHN extra set from training. Here
we achieved a classiﬁcation error of 29.82%. The improve-
ments on the NORB dataset was not as signiﬁcant as for
SVHN with the ADGM being slightly worse than VAT and
the SDGM being slightly better than VAT.
On SVHN the model trains to around 19% classiﬁcation
error in 100 epochs followed by a decline in convergence
speed. The NORB dataset is a signiﬁcantly smaller dataset
and the SDGM converges to around 12% in 100 epochs.
We also trained the NORB dataset on single images as op-
posed to image pairs (half the dataset) and achieved a clas-
siﬁcation error around 13% in 100 epochs.
For Gaussian input distributions, like the image data of
SVHN and NORB, we found the SDGM to be more sta-
ble than the ADGM.

Auxiliary Deep Generative Models

B. Variational bounds
In this appendix we give an overview of the variational ob-
jectives used. The generative model pθ(x, a, y, z) for the
Auxiliary Deep Generative Model and the Skip Deep Gen-
erative Model are deﬁned as

ADGM:
pθ(x, a, y, z) = pθ(x|y, z)pθ(a|x, y, z)p(y)p(z) .

(24)

(cid:90)

(cid:90)

(cid:20)

SDGM:
pθ(x, a, y, z) = pθ(x|a, y, z)pθ(a|x, y, z)p(y)p(z) . (25)
The lower bound −L(x, y) on the labeled log-likelihood is
deﬁned as

log p(x, y) = log

pθ(x, y, a, z)dzda

a

z

≥ Eqφ(a,z|x,y)

log

pθ(x, y, a, z)
qφ(a, z|x, y)

(cid:21)

(26)

≡ −L(x, y) ,

where qφ(a, z|x, y) = qφ(a|x)qφ(z|a, y, x). We deﬁne the
function f (·) to be f (x, y, a, z) = log pθ(x,y,a,z)
qφ(a,z|x,y). In the
lower bound for the unlabeled data −U(x) we treat the dis-
crete y3 as a latent variable. We rewrite the lower bound in
the form of Kingma et al. (2014):

log p(x) = log

pθ(x, y, a, z)dzda

≥ Eqφ(a,y,z|x) [f (·) − log qφ(y|a, x)]
= Eqφ(a|x)

(27)

A. Auxiliary model speciﬁcation
In this appendix we study the theoretical optimum of the
auxiliary variational bound found by functional derivatives
of the variational objective. In practice we will resort to
restricted deep network parameterized distributions. But
this analysis nevertheless shed some light on the proper-
ties of the optimum. Without loss of generality we con-
sider only auxiliary a and latent z: p(a, z) = p(z)p(a|z),
p(z) = f (z)/Z and q(a, z) = q(z|a)q(a). The results
can be extended to the full semi-supervised setting without
changing the overall conclusion. The variational bound for
the auxiliary model is

log Z ≥ Eq(a,z)

log

f (z)p(a|z)
q(z|a)q(a)

.

(21)

(cid:21)

We can now take the functional derivative of the bound
with respect to p(a|z). This gives the optimum p(a|z) =
q(a, z)/q(z), which in general is intractable because it re-

quires marginalization: q(z) =(cid:82) q(z|a)q(a)da.

One may also restrict the generative model to an unin-
formed a-model: p(a, z) = p(z)p(a). Optimizing with
respect to p(a) we ﬁnd p(a) = q(a). When we insert this
solution into the variational bound we get

(cid:20)

(cid:20)

(cid:21)

(cid:90)

q(a) Eq(z|a)

log

f (z)
q(z|a)

da .

(22)

The solution to the optimization with respect to q(a) will
simply be a δ-function at the value of a that optimizes the
variational bound for the z-model. So we fall back to a
model for z without the auxiliary as also noted by Agakov
and Barber (2004).
We have tested the uninformed auxiliary model in semi-
supervised learning for the benchmarks and we got com-
petitive results for MNIST but not for the two other bench-
marks. We attribute this to two factors: in semi-supervised
learning we add an additional classiﬁcation cost so that the
generic form of the objective is

(cid:20)

(cid:21)

(cid:90)

a

z

y

(cid:90)
(cid:88)
(cid:88)
(cid:2)−(cid:88)
(cid:124)
(cid:20)(cid:88)

y

y

y

Eqφ(a|x)

= Eqφ(a|x)

(cid:3)

(cid:123)(cid:122)

qφ(y|a, x)Eqφ(z|a,x) [f (·)] +
qφ(y|a, x) log qφ(y|a, x)

(cid:125)
qφ(y|a, x)Eqφ(z|a,x) [f (·)] +
(cid:21)

H(qφ(y|a,x))

H(qφ(y|a, x))

log Z ≥ Eq(a,z)

log

f (z)p(a)
q(z|a)q(a)

+ g(a)

,

(23)

≡ −U(x) ,

we keep p(a) ﬁxed to a zero mean unit variance Gaussian
and we use deep iid models for f (z), q(z|a) and q(a). This
taken together can lead to at least a local optimum which is
different from the collapse to the pure z-model.

where H(·) denotes the entropy. The objective function of
−L(x, y) and −U(x) are given in Eq. (12) and Eq. (13).

3y is assumed to be multinomial but the model can easily be

extended to different distributions.

Auxiliary Deep Generative Models

Acknowledgements
We thank Durk P. Kingma and Shakir Mohamed for help-
ful discussions. This research was supported by the Novo
Nordisk Foundation, Danish Innovation Foundation and
the NVIDIA Corporation with the donation of TITAN X
and Tesla K40 GPUs.

References
Agakov, F. and Barber, D. (2004). An Auxiliary Varia-
tional Method. In Neural Information Processing, vol-
ume 3316 of Lecture Notes in Computer Science, pages
561–566. Springer Berlin Heidelberg.

Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Good-
fellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y.
(2012). Theano: new features and speed improvements.
In Deep Learning and Unsupervised Feature Learning,
workshop at Neural Information Processing Systems.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015).
arXiv preprint

Importance Weighted Autoencoders.
arXiv:1509.00519.

Dieleman, S., Schlter, J., Raffel, C., Olson, E., Sønderby,
S. K., Nouri, D., van den Oord, A., and and, E. B. (2015).
Lasagne: First release.

Glorot, X. and Bengio, Y. (2010). Understanding the dif-
ﬁculty of training deep feedforward neural networks.
In Proceedings of the International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS10)., pages
249–256.

Ioffe, S. and Szegedy, C. (2015). Batch normalization: Ac-
celerating deep network training by reducing internal co-
variate shift. In Proceedings of International Conference
of Machine Learning, pages 448–456.

Kingma, D. and Ba, J. (2014). Adam: A Method
preprint

arXiv

for Stochastic Optimization.
arXiv:1412.6980.

Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling,
M. (2014). Semi-Supervised Learning with Deep Gener-
ative Models. In Proceedings of the International Con-
ference on Machine Learning, pages 3581–3589.

Kingma, Diederik P; Welling, M. (2013). Auto-Encoding

Variational Bayes. arXiv preprint arXiv:1312.6114.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. In Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition,
pages 2278–2324.

LeCun, Y., Huang, F. J., and Bottou, L. (2004). Learning
methods for generic object recognition with invariance
to pose and lighting. In Proceedings of the IEEE Com-
puter Society Conference on Computer Vision and Pat-
tern Recognition, pages 97–104.

Miyato, T., Maeda, S.-i., Koyama, M., Nakae, K., and Ishii,
S. (2015). Distributional Smoothing with Virtual Adver-
sarial Training. arXiv preprint arXiv:1507.00677.

Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and
Ng, A. Y. (2011). Reading digits in natural images with
In Deep Learning and
unsupervised feature learning.
Unsupervised Feature Learning, workshop at Neural In-
formation Processing Systems 2011.

Ranganath, R., Tang, L., Charlin, L., and Blei, D. M.
arXiv preprint

(2014). Deep exponential families.
arXiv:1411.2581.

Ranganath, R., Tran, D., and Blei, D. M.

Hierarchical variational models.
arXiv:1511.02386.

(2015).
arXiv preprint

Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and
Raiko, T. (2015). Semi-supervised learning with ladder
networks. In Advances in Neural Information Processing
Systems, pages 3532–3540.

Rezende, D. J. and Mohamed, S. (2015). Variational In-
ference with Normalizing Flows. In Proceedings of the
International Conference of Machine Learning, pages
1530–1538.

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014).
Stochastic Backpropagation and Approximate Infer-
arXiv preprint
ence in Deep Generative Models.
arXiv:1401.4082.

Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K.,
and Winther, O. (2016). Ladder variational autoen-
coders. arXiv preprint arXiv:1602.02282.

Tran, D., Ranganath, R., and Blei, D. M.

Variational Gaussian process.
arXiv:1511.06499.

(2015).
arXiv preprint

Uria, B., Murray, I., and Larochelle, H. (2013). Rnade:
The real-valued neural autoregressive density-estimator.
In Advances in Neural Information Processing Systems,
pages 2175–2183.

Valpola, H. (2014). From neural pca to deep unsupervised

learning. arXiv preprint arXiv:1411.7783.

van den Oord, A., Nal, K., and Kavukcuoglu, K.
(2016). Pixel recurrent neural networks. arXiv preprint
arXiv:1601.06759.

