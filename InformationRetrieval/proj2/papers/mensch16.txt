Dictionary Learning for Massive Matrix Factorization

Arthur Mensch
Parietal team, Inria, CEA, Paris-Saclay University. Neurospin, Gif-sur-Yvette, France

ARTHUR.MENSCH@M4X.ORG

Julien Mairal
Thoth team, Inria, Grenoble, France

JULIEN.MAIRAL@INRIA.FR

Bertrand Thirion
Gaël Varoquaux
Parietal team, Inria, CEA, Paris-Saclay University. Neurospin, Gif-sur-Yvette, France

BETRAND.THIRION@INRIA.FR
GAEL.VAROQUAUX@INRIA.FR

Abstract

Sparse matrix factorization is a popular tool to
obtain interpretable data decompositions, which
are also effective to perform data completion
or denoising.
Its applicability to large datasets
has been addressed with online and randomized
methods, that reduce the complexity in one of
the matrix dimension, but not in both of them.
In this paper, we tackle very large matrices in
both dimensions. We propose a new factorization
method that scales gracefully to terabyte-scale
datasets. Those could not be processed by pre-
vious algorithms in a reasonable amount of time.
We demonstrate the efﬁciency of our approach on
massive functional Magnetic Resonance Imaging
(fMRI) data, and on matrix completion problems
for recommender systems, where we obtain sig-
niﬁcant speed-ups compared to state-of-the art
coordinate descent methods.

Matrix factorization is a ﬂexible tool for uncovering latent
factors in low-rank or sparse models. For instance, build-
ing on low-rank structure, it has proven very powerful for
matrix completion, e.g. in recommender systems (Srebro
et al., 2004; Candès & Recht, 2009).
In signal process-
ing and computer vision, matrix factorization with a sparse
regularization is often called dictionary learning and has
proven very effective for denoising and visual feature en-
coding (see Mairal, 2014, for a review). It is also ﬂexible
enough to accommodate a large set of constraints and regu-
larizations, and has gained signiﬁcant attention in scientiﬁc
domains where interpretability is a key aspect, such as ge-

Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume
48. Copyright 2016 by the author(s).

netics and neuroscience (Varoquaux et al., 2011).
As a widely-used model, the literature of matrix factoriza-
tion is very rich and two main classes of formulations have
emerged. The ﬁrst one addresses an optimization prob-
lem involving a convex penalty, such as the trace or max
norms (Srebro et al., 2004). These penalties promote low-
rank structures, have strong theoretical guarantees (Candès
& Recht, 2009), but they do not encourage sparse factors
and lack scalability for very-large datasets. For these rea-
sons, our paper is focused on a second type of approach,
that relies on nonconvex optimization. Speciﬁcally, the mo-
tivation of our work originally came from the need to ana-
lyze huge-scale fMRI datasets, and the difﬁculty of current
algorithms to process them.
To gain scalability, stochastic (or online) optimization
methods have been developed; unlike classical alternate
minimization procedures, they learn matrix decomposi-
tions by observing a single matrix column (or row) at each
iteration. In other words, they stream data along one matrix
dimension. Their cost per iteration is signiﬁcantly reduced,
leading to faster convergence in various practical contexts.
More precisely, two approaches have been particularly suc-
cessful: stochastic gradient descent (see Bottou, 2010) has
been widely used in recommender systems (see Bell & Ko-
ren, 2007; Rendle & Schmidt-Thieme, 2008; Rendle, 2010;
Blondel et al., 2015, and references therein), and stochastic
majorization-minimization methods for dictionary learning
with sparse and/or structured regularization (Mairal et al.,
2010; Mairal, 2013). Yet, stochastic algorithms for dictio-
nary learning are currently unable to deal efﬁciently with
matrices that are large in both dimensions.
In a somehow orthogonal way, the growth of dataset size
has proven to be manageable by randomized methods,
that exploit random projections (Johnson & Lindenstrauss,
1984; Bingham & Mannila, 2001) to reduce data dimension

Dictionary Learning for Massive Matrix Factorization

without deteriorating signal content. Due to the way they
are generated, large-scale datasets generally have an intrin-
sic dimension that is signiﬁcantly smaller than their ambi-
ent dimension. Biological datasets (McKeown et al., 1998)
and physical acquisitions with an underlying sparse struc-
ture enabling compressed sensing (Candès & Tao, 2006)
are good examples. In this context, matrix factorization can
be performed by using random summaries of coefﬁcients.
Recently, those have been used to compute PCA (Halko
et al., 2009), a classical matrix decomposition technique.
Yet, using random projections as a pre-processing step is
not appealing in our applicative context since the factors
learned on reduced data loses interpretability.

Main contribution.
In this paper, we propose a dictio-
nary learning algorithm that (i) scales both in the signal
dimension (number of rows) and number of signals (num-
ber of columns), (ii) deals with various structured sparse
regularization penalties, (iii) handles missing values, and
(iv) provides an explicit dictionary with easy interpretation.
As such, it is non-trivial extension of the online dictionary
learning method of Mairal et al. (2010), where, at every it-
eration, signals are partially observed with a random mask,
and with low-complexity update rules that depend on the
(small) mask size instead of the signal size.
To the best of our knowledge, our algorithm is the ﬁrst
that enjoys all aforementioned features; in particular, we
are not aware of any other dictionary learning algorithm
that is scalable in both matrix dimensions. For instance,
Pourkamali-Anaraki et al. (2015) use random projection
with k-SVD, a batch dictionary learning algorithm (Aharon
et al., 2006) that does not scale well in the number of train-
ing signals. Online matrix decomposition in the context of
missing values was also proposed by Szabó et al. (2011),
but without scalability in the signal (row) size.
On a massive fMRI dataset (2TB, n = 2.4·106, p = 2·105),
we were able to learn interpretable dictionaries in about
10 hours on a single workstation, an order of magnitude
faster than the online approach of Mairal et al. (2010). On
collaborative ﬁltering experiments, where sparsity is not
needed, our algorithm performs favorably well compared
to state-of-the-art coordinate descent methods. In both ex-
periments, beneﬁts for the practitioner were signiﬁcant.

1. Background on Dictionary Learning
In this section, we introduce dictionary learning as a matrix
factorization problem, and present stochastic algorithms
that observe one column (or a minibatch) at every iteration.

1.1. Problem Statement
The goal of matrix factorization is to decompose a matrix

X ∈ Rp×n – typically n signals of dimension p – as a
product of two smaller matrices:

X ≈ DA with D ∈ Rp×k, A ∈ Rk×n,

(1)

with potential sparsity or structure requirements on D
and A. In statistical signal applications, this is often a dic-
tionary learning problem, enforcing sparse coefﬁcients A.
In such a case, we call D the “dictionary” and A the sparse
codes. We use this terminology throughout the paper.
Learning the dictionary is typically performed by minimiz-
ing a quadratic data-ﬁtting term, with constraints and/or
penalties over the code and the dictionary:

n(cid:88)

(cid:13)(cid:13)xi − Dαi

(cid:13)(cid:13)2

1
2

min
D∈C

2 + λ Ω(αi),

(2)

i=1

A=[α1,...,αn]∈Rk×n
where C is a convex set of Rp×k, and a Ω : Rp → R is a
penalty over the code, to enforce structure or sparsity. In
large n and large p settings, typical in recommender sys-
tems, this problem is solved via block coordinate descent,
which boils down to alternating least squares if regulariza-
tions on D and α are quadratic (Hastie et al., 2014).

Constraints and penalties. The constraint set C is tra-
ditionally a technical constraint ensuring that the coefﬁ-
cients α do not vanish, making the effect of the penalty Ω
disappear. However, other constraints can also be used to
enforce sparsity or structure on the dictionary (see Varo-
quaux et al., 2013). In our paper,C is the Cartesian product
of a (cid:96)1 or (cid:96)2 norm ball:
C = {D ∈ Rp×k s.t. ψ(dj) ≤ 1 ∀j = 1, . . . , k}, (3)
where D = [d1, . . . , dk] and ψ = (cid:107) · (cid:107)1 or ψ = (cid:107) · (cid:107)2. The
choice of ψ and Ω typically offers some ﬂexibility in the
regularization effect that is desired for a speciﬁc problem;
for instance, classical dictionary learning uses ψ = (cid:107) · (cid:107)2
and Ω = (cid:107)·(cid:107)1, leading to sparse coefﬁcients α, whereas our
experiments on fMRI uses ψ = (cid:107)·(cid:107)1 and Ω = (cid:107)·(cid:107)2
2, leading
to sparse dictionary elements dj that can be interpreted as
brain activation maps.

1.2. Streaming Signals with Online Algorithms
In stochastic optimization, the number of signals n is as-
sumed to be large (or potentially inﬁnite), and the dictio-
nary D can be written as a solution of

(cid:2)l(x, D)(cid:3)

(4)

f (D) = Ex

D∈C f (D) where
min
1
2(cid:107)x − Dα(cid:107)2

l(x, D) = min
α∈Rk

2 + λ Ω(α),

where the signals x are assumed to be i.i.d. samples from
an unknown probability distribution. Based on this formu-

Dictionary Learning for Massive Matrix Factorization

lation, Mairal et al. (2010) have introduced an online dic-
tionary learning approach that draws a single signal xt at it-
eration t (or a minibatch), and computes its sparse code αt
using the current dictionary Dt−1 according to

αt ← argmin
α∈Rk

1
2(cid:107)xt − Dt−1α(cid:107)2

2 + λ Ω(α).

(5)

Then, the dictionary is updated by approximately minimiz-
ing the following surrogate function

t(cid:88)

i=1

(cid:13)(cid:13)xi − Dαi

(cid:13)(cid:13)2

1
2

gt(D) =

1
t

2 + λ Ω(αi),

(6)

which involves the sequence of past signals x1, . . . , xt and
the sparse codes α1, . . . , αt that were computed in the past
iterations of the algorithm. The function gt is called a “sur-
rogate” in the sense that it only approximates the objec-
tive f. In fact, it is possible to show that it converges to
a locally tight upper-bound of the objective, and that mini-
mizing gt at each iteration asymptotically provides a sta-
tionary point of the original optimization problem. The
underlying principle is that of majorization-minimization,
used in a stochastic fashion (Mairal, 2013).
One key to obtain efﬁcient dictionary updates is the obser-
vation that the surrogate gt can be summarized by a few
sufﬁcient statistics that are updated at every iteration. In
other words, it is possible to describe gt without explicitly
storing the past signals xi and codes αi for i ≤ t. Indeed,
we may deﬁne two matrices Bt ∈ Rp×k and Ct ∈ Rk×k
(7)

t(cid:88)

t(cid:88)

xiα(cid:62)
i ,

αiα(cid:62)

i

Ct =

Bt =

1
t

i=1

1
t

i=1

and the surrogate function is then written:

gt(D) =

1
2

Tr(D(cid:62)DCt − D(cid:62)Bt) +

λ
t

The gradient of gt can be computed as

t(cid:88)

i=1

Ω(αi).

(8)

∇Dgt(D) = DCt − Bt.

(9)

Minimization of gt is performed using block coordinate de-
scent on the columns of D. In practice, the following up-
dates are successively performed by cycling over the dic-
tionary elements dj for j = 1, . . . , k

dj ← Projψ(.)≤1

dj −

1

Ct[j, j]∇dj gt(D)

,

(10)

where Proj denotes the Euclidean projection over the con-
straint norm constraint ψ. It can be shown that this update
corresponds to minimizing gt with respect to dj when ﬁx-
ing the other dictionary elements (see Mairal et al., 2010).

(cid:20)

(cid:21)

1.3. Handling Missing Values
Factorization of matrices with missing value have raised a
signiﬁcant interest in signal processing and machine learn-
ing, especially as a solution for recommender systems. In
the context of dictionary learning, a similar effort has been
made by Szabó et al. (2011) to adapt the framework to
missing values. Formally, a mask M, represented as a bi-
nary diagonal matrix in {0, 1}p×p, is associated with every
signal x, such that the algorithm can only observe the prod-
uct Mtxt at iteration t instead of a full signal xt. In this
setting, we naturally derive the following objective

D∈C f (D) where
min
l(x, M, D) = min
α∈Rk

f (D) = Ex,M
p

2Tr M(cid:107)M(x − Dα)(cid:107)2

2 + λΩ(α),

(cid:2)l(x, M, D)(cid:3)

(11)

where the pairs (x, M) are drawn from the (unknown)
data distribution. Adapting the online algorithm of Mairal
et al. (2010) would consist of drawing a sequence of
pairs (xt, Mt), and building the surrogate

t(cid:88)

i=1

(cid:13)(cid:13)Mi(xi − Dαi)(cid:13)(cid:13)2

p
2si

gt(D) =

1
t

2 + λ Ω(αi), (12)

where si = Tr Mi is the size of the mask and

αi ∈ argmin
α∈Rk

p
2si(cid:107)Mi(xi − Di−1α)(cid:107)2

2 + λ Ω(α).

(13)

Unfortunately, this surrogate cannot be summarized by a
few sufﬁcient statistics due to the masks Mi: some approx-
imations are required. This is the approach chosen by Sz-
abó et al. (2011). Nevertheless, the complexity of their up-
date rules is linear in the full signal size p, which makes
them unadapted to the large-p regime that we consider.

2. Dictionary Learning for Massive Data
Using the formalism exposed above, we now consider the
problem of factorizing a large matrix X in Rp×n into two
factors D in Rp×k and A in Rk×n with the following set-
ting: both n and p are large (greater than 100 000 up to sev-
eral millions), whereas k is reasonable (smaller than 1 000
and often near 100), which is not the standard dictionary-
learning setting; some entries of X may be missing. Our
objective is to recover a good dictionary D taking into ac-
count appropriate regularization.
To achieve our goal, we propose to use an objective akin
to (11), where the masks are now random variables inde-
pendant from the samples.
In other words, we want to
combine ideas of online dictionary learning with random
subsampling, in a principled manner. This leads us to con-
sider an inﬁnite stream of samples (Mtxt)t≥0, where the
signals xt are i.i.d. samples from the data distribution – that

Dictionary Learning for Massive Matrix Factorization

is, a column of X selected at random – and Mt “selects” a
random subset of observed entries in X. This setting can
accommodate missing entries, never selected by the mask,
and only requires loading a subset of xt at each iteration.
The main justiﬁcation for choosing this objective function
is that in the large sample regime p (cid:29) k that we con-
sider, computing the code αi using only a random subset
of the data xt according to (13) is a good approximation
of the code that may be computed with the full vector xt
in (5). This of course requires choosing a mask that is
large enough; in the fMRI dataset, a subsampling factor
of about r = 10 – that is only 10% of the entries of xt are
observed – resulted in a similar 10× speed-up (see experi-
mental section) to achieve the same accuracy as the original
approach without subsampling. This point of view also jus-
tiﬁes the natural scaling factor
An efﬁcient algorithm must address two challenges: (i) per-
forming dictionary updates that do not depend on p but
only on the mask size; (ii) ﬁnding an approximate surro-
gate function that can be summarized by a few sufﬁcient
statistics. We provide a solution to these two issues in the
next subsections and present the method in Algorithm 1.

Tr M introduced in (11).

p

2.1. Approximate Surrogate Function
To approximate the surrogate (8) from αt computed in
(13), we consider ht deﬁned by

with the same matrix Ct as in (8), which is updated as

and to replace Bt in (8) by the matrix

1
t

1 −

Ct ←

(cid:16)
(cid:16) t(cid:88)
a diagonal matrix, (cid:80)t

Bt =

i=1

Ct−1 +

(cid:17)
(cid:17)−1 t(cid:88)

i=1

Mi

Mixiα(cid:62)
i ,

which is the same as (7) when Mi = I. Since Mi is
i=1 Mi is also diagonal and simply
“counts” how many times a row has been seen by the al-
gorithm. Bt thus behaves like Ex[xα(x, Dt)(cid:62)] for large
t, as in the fully-observed algorithm. By design, only rows
of Bt selected by the mask differ from Bt−1. The update
can therefore be achieved in O(sik) operations:

(cid:16) t(cid:88)

i=1

(cid:17)−1(cid:0)Mtxtα(cid:62)

Bt = Bt−1+

Mi

t − MtBt−1

(cid:1) (17)

This only requires keeping in memory the diagonal matrix
i=1 Mi, and updating the rows of Bt−1 selected by the
mask. All operations only depend on the mask size si in-
stead of the signal size p.

(cid:80)t

2.2. Efﬁcient Dictionary Update Rules
With a surrogate function in hand, we now describe how to
update the codes α and the dictionary D when only par-
tial access to data is possible. The complexity for comput-
ing the sparse codes αt is obviously independent from p
since (13) consists in solving a reduced penalized linear re-
gression of Mtxt in Rst on MtDt−1 in Rst×k. Thus, we
focus here on dictionary update rules.
The naive dictionary update (18) has complexity O(kp)
due to the matrix-vector multiplication for computing
∇dj gt(D). Reducing the single iteration complexity of a
factor p
requires reducing the dimensionality of the dic-
st
tionary update phase. We propose two strategies to achieve
that, both using block coordinate descent, by considering

dj ← Projψ(.)≤1

dj −

1

Ct[j, j]

Mt∇dj ht(D)

,

(18)

where Mt∇dj ht(D) is the partial derivative of ht with re-
spect to the j-th column and rows selected by the mask.

Gradient step. The update (18) represents a classical
block coordinate descent step involving particular blocks.
Following Mairal et al. (2010), we perform one cycle over
the columns warm-started on Dt−1. Formally, the gradient
step without projection for the j-th component consists of
updating the vector dj

(cid:20)

(cid:21)

= dj −

1

Ct[j, j]

Mt∇dj ht(D)
(MtDct

j − Mtbt
j),

(19)

j, bt

where ct
j are the j-th columns of Ct, Bt respectively.
The update has complexity O(kst) since it only involves st
rows of D and only st entries of dj have changed.

Projection step. Block coordinate descent algorithms re-
quire orthogonal projections onto the constraint set C. In
our case, this amounts to the projection step on the unit ball
corresponding to the norm ψ in (18). The complexity of
such a projection is usually O(p) both for (cid:96)2 and (cid:96)1-norms
(see Duchi et al., 2008). We consider here two strategies.

Exact lazy projection for (cid:96)2. When ψ = (cid:96)2, it is pos-
sible to perform the projection implicitly with complexity
O(st). The computational trick is to notice that the projec-
tion amounts to a simple rescaling operation

dj

,

(20)

dj ←

max(1,(cid:107)dj(cid:107)2)
which may have low complexity if
elements dj

the dictionary
are stored in memory as a product

ht(D) =

1
2

Tr(D(cid:62)DCt−D(cid:62)Bt)+

λ
t

si
p

Ω(αi) (14)

dj ← dj −

1

Ct[j, j]

t(cid:88)

i=1

αtα(cid:62)
t ,

1
t

(15)

(16)

Dictionary Learning for Massive Matrix Factorization

B0 ← 0 ∈ Rp×k;
t ← 1;

Procedure 1 Dictionary Learning for Massive Data
Input: Initial dictionary: D0 ∈ Rp×k, tolerance: 
C0 ← 0 ∈ Rk×k;
E0 ← 0 ∈ Rp×p (diagonal);
repeat
Draw a pair (xt, Mt);
αt←argminα
Et ← Et + Mt;
At ← (1 − 1
Bt ← Bt−1 + E−1
Dt ← dictionary_update(Bt, Ct, Dt−1, Mt);
until | ht−1(Dt−1)
Output: D

(cid:62);
t αtαt
(cid:62)
− MtBt−1);
t (Mtxtαt

2(cid:107)Mt(xt−Dt−1α)(cid:107)2

ht(Dt) − 1| < 

t )At−1 + 1

2+λ Tr Mt

1

p Ω(α);

dj=fj/ max(1, lj) where fj is in Rp and lj is a rescal-
ing coefﬁcient such that lj = (cid:107)fj(cid:107)2. We code the gradient
step (19) followed by (cid:96)2-ball projection by the updates

2

max(1, lj)

nj ← (cid:107)Mjfj(cid:107)2
fj ← fj −
l2
j − nj + (cid:107)Mjfj(cid:107)2
lj ←

(cid:113)

Ct[j, j]

2

(MtDct

j − Mtbt
j)

(21)

Note that the update of fj corresponds to the gradient step
without projection (19) which costs O(kst), whereas the
norm of fj is updated in O(st) operations. The compu-
tational complexity is thus independent of p and the only
price to pay is to rescale the dictionary elements on the ﬂy,
each time we need access to them.

Exact lazy projection for (cid:96)1. The case of (cid:96)1 is slightly
different but can be handled in a similar manner, by stor-
ing an additional scalar lj for each dictionary element dj.
More precisely, we store a vector fj in Rp such that
dj = Projψ(.)≤1[fj], and a classical result (see Duchi
et al., 2008) states that there exists a scalar lj such that
dj = Slj [fj], Sλ(u) = sign(u). max(|u| − λ, 0)
where Sλ is the soft-thresholding operator, applied elemen-
twise to the entries of fj. Similar to the case (cid:96)2, the “lazy”
projection consists of tracking the coefﬁcient lj for each
dictionary element and updating it after each gradient step,
which only involves st coefﬁcients. For such sparse up-
dates followed by a projection onto the (cid:96)1-ball, Duchi et al.
(2008) proposed an algorithm to ﬁnd the threshold lj in
O(st log(p)) operations. The lazy algorithm involves us-
ing particular data structures such as red-black trees and is
not easy to implement; this motivated us to investigate an-
other simple heuristic that also performs well in practice.

(22)

Approximate low-dimension projection. The heuris-
tic consists in performing the projection by forcing the

Procedure 2 Dictionary Update
Input: B, C, D, M
for j ∈ 1, . . . , k do
dj ← dj − 1
if approximate projection then

C[j,j] (MDcj − Mbj);

vj ← ProjTj [Mdj],
(see main text for the deﬁnition of Tj);
dj ← dj + Mvj − Mdj;
or dj ← Projψ(.)≤1 [dj];

else if exact (lazy) projection then

end if
end for

coefﬁcients outside the mask not to change. This re-
sults in the orthogonal projection of each dj on Tt,j =
{d s.t. ψ(d) ≤ 1, (I − Mt)d = (I − Mt)dt−1
}, which
is a subset of the original constraint set ψ(·) ≤ 1.
All the computations require only 4 matrices kept in mem-
ory B, C, D, E with additional F, l matrices and vectors
for the exact projection case, as summarized in Alg. 1.

j

n(cid:88)

n(cid:88)

2.3. Discussion
Relation to classical matrix completion formulation.
Our model is related to the classical (cid:96)2-penalized matrix
completion model (e.g. Bell & Koren, 2007) we rewrite

1

2

i=1

i=1

Mi)

2 + λ(cid:107)(

(cid:107)Mi(xi − D(cid:62)αi)(cid:107)2

2 + λsi(cid:107)αi(cid:107)2

2 D(cid:107)2
(23)
With quadratic regularization on D and A – that is, using
Ω = (cid:107).(cid:107)2
2 and ψ = (cid:107).(cid:107)2 – (11) only differs in that it uses
a penalization on D instead of a constraint. Srebro et al.
(2004) introduced the trace-norm regularization to solve a
convex problem equivalent to (23). The major difference
is that we adopt a non-convex optimization strategy, thus
losing the beneﬁts of convexity, but gaining on the other
hand the possibility of using stochastic optimization.

Practical considerations. Our algorithm can be slightly
modiﬁed to use weights wt that differ from 1
t for B and
C, as advocated by Mairal (2013). It also proves beneﬁcial
to perform code computation on mini-batches of masked
samples. Update of the dictionary is performed on the rows
that are seen at least once in the masks (Mt)batch.

3. Experiments
The proposed algorithm was designed to handle mas-
sive datasets: masking data enables streaming a sequence
(Mtxt)t instead of (xt)t, reducing single-iteration compu-
pE(Tr M),
tational complexity and IO stress of a factor r =
while accessing an accurate description of the data. Hence,

Dictionary Learning for Massive Matrix Factorization

Figure 1. Acceleration of sparse matrix factorization with random subsampling on the HCP dataset (2TB). Reducing streamed data
with stochastic masks permits 10× speed-ups without deteriorating goodness of ﬁt on test data nor alterating sparsity of ﬁnal dictionary.

we analyze in detail how our algorithm improves perfor-
mance for sparse decomposition of fMRI datasets. More-
over, as it relies on data masks, our algorithm is well suited
for matrix completion, to reconstruct a data stream (xt)t
from the masked stream (Mtxt)t. We demonstrate the ac-
curacy of our algorithm on explicit recommender systems
and show considerable computational speed-ups compared
to an efﬁcient coordinate-descent based algorithm.
We use scikit-learn (Pedregosa et al., 2011) in experiments,
and have released a python package1 for reproducibility.

3.1. Sparse Matrix Factorization for fMRI
Context. Matrix factorization has long been used on
functional Magnetic Resonance Imaging (McKeown et al.,
1998). Data are temporal series of 3D images of brain ac-
tivity, to decompose in spatial modes capturing regions that
activate together. The matrices to decompose are dense
and heavily redundant, both spatially and temporally: close
voxels and successive records are correlated. Data can be
huge: we use the whole HCP dataset (Van Essen et al.,
2013), with n = 2.4· 106 (2000 records, 1 200 time points)
and p = 2 · 105, totaling 2 TB of dense data.
Interesting dictionaries for neuroimaging capture spatially-
localized components, with a few brain regions. This can
be obtained by enforcing sparsity on the dictionary: in our
formalism, this is achieved with (cid:96)1-ball projection for D.
We set C = Bk
2. Historically, such decom-
position have been obtained with the classical dictionary
learning objective on transposed data (Varoquaux et al.,
2013):
the code A holds sparse spatial maps and voxel
time-series are streamed. However, given the size of n for
our dataset, this method is not usable in practice.
Handling such volume of data sets new constraints. First,
efﬁcient disk access becomes critical for speed. In our case,
learning the dictionary is done by accessing the data in row
batches, which is coherent with fMRI data storage: no time
is lost seeking data on disk. Second, reducing IO load on

1 , and Ω = (cid:107) · (cid:107)2

1http://github.com/arthurmensch/modl

the storage is also crucial, as it lifts bottlenecks that appear
when many processes access the same storage at the same
time, e.g. during cross-validation on λ within a supervised
pipeline. Our approach reduces disk usage by a factor r.
Finally, parallel methods based on message passing, such
as asynchronous coordinate descent, are unlikely to be efﬁ-
cient given the network / disk bandwidth that each process
requires to load data. This makes it crucial to design efﬁ-
cient sequential algorithms.

Experiment We quantify the effect of random subsam-
pling for sparse matrix factorization, in term of speed and
accuracy. A natural performance evaluation is to measure
an empirical estimate of the loss l deﬁned in Eq. 4 from
unseen data, to rule out any overﬁtting effect. For this, we
evaluate l on a test set (xi)i<N . Pratically, we sample (xt)t
in a pseudo-random manner: we randomly select a record,
from where we select a random batch of rows xt – we use
a batch size of 40, empirically found to be efﬁcient. We
load Mtxt in memory and perform an iteration of the algo-
rithm. The mask sequence is sampled by breaking random
permutation vectors into chunks of size p/r.

Results Fig. 1(a) compares our algorithm with subsam-
pling ratios r in {4, 8, 12} to vanilla online dictionary
learning algorithm (r = 1), plotting trajectories of the
test objective against real CPU time. There is no obvious
choice of λ due to the unsupervised nature of the problem:
we use 10−3 and 10−4, that bounds the range of λ provid-
ing interpretable dictionaries.
First, we observe the convergence of the objective func-
tion for all tested r, providing evidence that the approx-
imations made in the derivation of update rules does not
break convergence for such r. Fig. 1(b) shows the validity
of the obtained dictionary relative to the reference output:
both objective function and (cid:96)1/(cid:96)2 ratio – the relevant value
to measure sparsity in our setting – are comparable to the
baseline values, up to r = 8. For high regularization and
r = 12, our algorithm tends to yield somewhat sparser so-
lutions (5% lower (cid:96)1/(cid:96)2) than the original algorithm, due to
the approximate (cid:96)1-projection we perform. Obtained maps

.1h1h10h100h2.302.322.342.362.382.402.422.442.46Objectivevalueontestset×108λ=10−3OriginalonlinealgorithmProposedreductionfactorr(a)Convergencespeed4812Noreduction.1h1h10h100hCPUtime2.202.252.302.352.40×108λ=10−4-0.5%0%0.5%Finalobjectivedeviation(relative)(Lessisbetter)(b)Decompositionquality10−210−3Regularizationλ1001000‘1‘2(D)Dictionary Learning for Massive Matrix Factorization

Original online algorithm

1 full epoch

Original online algorithm

1

24 epoch

Proposed algorithm

1

2 epoch, reduction r=12

235 h run time

10 h run time

10 h run time

Figure 2. Brain atlases: outlines of each map at half the maximum value (λ = 10−4). Left: the reference algorithm on the full dataset.
Middle: the reference algorithm on a twentieth of the dataset. Right: the proposed algorithm with a similar run time: half the dataset
and r = 9. Compared to a full run of the baseline algorithm, the ﬁgure explore two possible strategies to decrease computation time:
processing less data (middle), or our approach (right). Our approach achieves a result closer to the gold standard in a given time budget.

performance (e.g. in classiﬁcation) ceases to improve.

3.2. Collaborative Filtering with Missing Data
We validate the performance of the proposed algorithm on
recommender systems for explicit feedback, a well-studied
matrix completion problem. We evaluate the scalability of
our method on datasets of different dimension: MovieLens
1M, MovieLens 10M, and 140M ratings Netﬂix dataset.
We compare our algorithm to a coordinate-descent based
method (Yu et al., 2012), that provides state-of-the art con-
vergence time performance on our largest dataset. Al-
though stochastic gradient descent methods for matrix fac-
torization can provide slightly better single-run perfor-
mance (Takács et al., 2009), these are notoriously hard to
tune and require a precise grid search to uncover a working
schedule of learning rates. In contrast, coordinate descent
methods do not require any hyper-parameter setting and are
therefore more efﬁcient in practice. We benchmarked var-
ious recommender-system codes (MyMediaLite, LibFM,
SoftImpute, spira2), and chose coordinate descent algo-
rithm from spira as it was by far the fastest.

Completion from dictionary Dt. We stream user ratings
to our algorithm: p is the number of movies and n is the
number of users. As n (cid:29) p on Netﬂix dataset, this in-
creases the beneﬁt of using an online method. We have ob-
served comparable prediction performance streaming item
ratings. Past the ﬁrst epoch, at iteration t, every column i
of X can be predicted by the last code αl(i,t) that was com-
puted from this column at iteration l(i, t). At iteration t, for
all i < [n], xpred
i = Dαl(i,t). Prediction thus only requires
an additional matrix computation after the factorization.

Preprocessing. Successful prediction should take into
account user and item biases. We compute these biases
on train data following Hastie et al. (2014) (alternated de-

2https://github.com/mblondel/spira

Figure 3. Evolution of objective function with epochs for three
reduction factors. Learning speed per epoch is little reduced by
stochastic subsampling, despite the speed-up factor it provides.

still proves as interpretable as with baseline algorithm.
Our algorithm proves much faster than the original one in
ﬁnding a good dictionary. Single iteration time is indeed
reduced by a factor r, which enables our algorithm to go
over a single epoch r times faster than the vanilla algo-
rithm and capture the variability of the dataset earlier. To
quantify speed-ups, we plot the empirical objective value
of D against the number of observed records in Fig. 3.
For r ≤ 12, increasing r little reduces convergence speed
per epoch: random subsampling does not shrink much the
quantity of information learned at each iteration.
This brings a near ×r speed-up factor: for high and low
regularization respectively, our algorithm converges in 3
and 10 hours with subsampling factor r = 12, whereas the
vanilla online algorithm requires about 30 and 100 hours.
Qualitatively, Fig. 2 shows that with the same time budget,
the proposed reduction approach with r = 12 on half of
the data gives better results than processing a small frac-
tion of the data without reduction: segmented regions are
less noisy and closer to processing the full data.
These results advocates the use of a subsampling rate of
r ≈ 10 in this setting. When sparse matrix decomposition
is part of a supervised pipeline with scoring capabilities,
it is possible to ﬁnd r efﬁciently: start by setting it derea-
sonably high and decrease it geometrically until supervised

1001000Epoch4000Records2.162.172.182.192.202.212.222.232.24Objectivevalueontestset×108λ=10−4Noreduction(originalalg.)r=4r=8r=12Dictionary Learning for Massive Matrix Factorization

Figure 4. Learning speed for collaborative ﬁltering for datasets of different size: the larger the dataset, the greater our speed-up.

Table 1. Comparison of performance and convergence time
for online masked matrix factorization and coordinate descent
method. Convergence time: score is under 0.1% deviation from
ﬁnal root mean squared error on test set – 5 runs average. CD:
coordinate descent; MODL: masked online dictionary learning.

Dataset

ML 1M
ML 10M
NF (140M)

Test RMSE
CD MODL CD
6 s
0.872
223 s
0.802
0.938
1714 s 256 s

0.866
0.799
0.934

Convergence time Speed

MODL
8 s
60 s

-up
×0.75
×3.7
×6.8

biasing). We use them to center the samples (xt)t that are
streamed to the algorithm, and to perform ﬁnal prediction.

Tools and experiments. Both baseline and proposed al-
gorithm are implemented in a computationally optimal
way, enabling fair comparison based on CPU time. Bench-
marks were run using a single 2.7 GHz Xeon CPU, with
a 30 components dictionary. For Movielens datasets, we
use a random 25% of data for test and the rest for training.
We average results on ﬁve train/test split for MovieLens in
Table 1. On Netﬂix, the probe dataset is used for testing.
Regularization parameter λ is set by cross-validation on the
training set: the training data is split 3 times, keeping 33%
of Movielens datasets for evaluation and 1% for Netﬂix,
and grid search is performed on 15 values of λ between
10−2 and 10. We assess the quality of obtained decom-
position by measuring the root mean square error (RMSE)
between prediction on the test set and ground truth. We use
mini-batches of size n

100.

deviation from ﬁnal RMSE), which makes our method 6.8
times faster than coordinate descent. Moreover, the relative
performance of our algorithm increases with dataset size.
Indeed, as datasets grow, less epochs are needed for our
algorithm to reach convergence (Fig. 4). This is a signiﬁ-
cant advantage over coordinate descent, that requires a sta-
ble number of cycle on coordinates to reach convergence,
regardless of dataset size. The algorithm with partial pro-
jection performs slightly better. This can be explained by
the extra regularization on (Dt)t brought by this heuristic.

Learning weights. Unlike SGD, and similar to the
vanilla online dictionary learning algorithm, our method
does not critically suffer from hyper-parameter tuning. We
tried weights wt = 1
tβ as described in Sec. 2.3, and ob-
served that a range of β yields fast convergence. Theo-
retically, Mairal (2013) shows that stochastic majorization-
minimization converges when β ∈ (.75, 1]. We verify
this empirically, and obtain optimal convergence speed for
β ∈ [.85, 0.95]. (Fig. 5). We report results for β = 0.9.

Figure 5. Learning weights: on two different datasets, optimal
convergence is obtained for β ∈ [.85, .95], predicted by theory.

Results. We report the evolution of test RMSE along time
in Fig. 4, along with its value at convergence and numerical
convergence time in Table 1. Benchmarks are performed
on the ﬁnal run, after selection of parameter λ.
The two variants of the proposed method converge toward
a solution that is at least as good as that of coordinate de-
scent, and slightly better on Movielens 10M and Netﬂix.
Our algorithm brings a substantial performance improve-
ment on medium and large scale datasets. On Netﬂix, con-
vergence is almost reached in 4 minutes (score under 0.1%

4. Conclusion
Whether it
is sensor data, as fMRI, or e-commerce
databases, sample sizes and number of features are rapidly
growing, rendering current matrix factorization approaches
intractable. We have introduced a online algorithm that
leverages random feature subsampling, giving up to 8-fold
speed and memory gains on large data. Datasets are getting
bigger, and they often come with more redundancies. Such
approaches blending online and randomized methods will
yield even larger speed-ups on next-generation data.

0.1s1s10s0.870.880.890.900.91RMSEontestsetMovieLens1M1s10s100s0.800.810.820.830.840.850.86MovieLens10M100s1000sCPUtime0.930.940.950.960.970.980.99Netﬂix(140M)CoordinatedescentProposed(fullprojection)Proposed(partialprojection)11040Epoch0.800.810.820.830.840.850.860.87RMSEontestsetLearningrateβ0.750.780.810.830.860.890.920.940.971.00MovieLens10M.1110200.930.940.950.960.970.980.99NetﬂixDictionary Learning for Massive Matrix Factorization

Acknowledgements
The research leading to these results was supported by
the ANR (MACARON project, ANR-14-CE23-0003-01
– NiConnect project, ANR-11-BINF-0004NiConnect) and
has received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) under grant
agreement no. 604102 (HBP).

References
Aharon, Michal, Elad, Michael, and Bruckstein, Alfred. k-
SVD: An algorithm for designing overcomplete dictio-
naries for sparse representation. IEEE Transactions on
Signal Processing, 54(11):4311–4322, 2006.

Bell, Robert M. and Koren, Yehuda. Lessons from the
Netﬂix prize challenge. ACM SIGKDD Explorations
Newsletter, 9(2):75–79, 2007.

Bingham, Ella and Mannila, Heikki. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 245–250. ACM, 2001.

Blondel, Mathieu, Fujino, Akinori, and Ueda, Naonori.
In Machine Learning
Convex factorization machines.
and Knowledge Discovery in Databases, pp. 19–35.
Springer, 2015.

Bottou, Léon. Large-scale machine learning with stochas-
tic gradient descent. In Proceedings of COMPSTAT, pp.
177–186. Springer, 2010.

Candès, Emmanuel J. and Recht, Benjamin. Exact ma-
trix completion via convex optimization. Foundations
of Computational Mathematics, 9(6):717–772, 2009.

Candès, Emmanuel J. and Tao, Terence. Near-optimal sig-
nal recovery from random projections: Universal encod-
ing strategies? Information Theory, IEEE Transactions
on, 52(12):5406–5425, 2006.

Duchi, John, Shalev-Shwartz, Shai, Singer, Yoram, and
Chandra, Tushar. Efﬁcient projections onto the l 1-
ball for learning in high dimensions. In Proceedings of
the International Conference on Machine Learning, pp.
272–279. ACM, 2008.

Halko, Nathan, Martinsson, Per-Gunnar, and Tropp,
Joel A. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decom-
positions. arXiv:0909.4061 [math], 2009.

Hastie, Trevor, Mazumder, Rahul, Lee, Jason, and Zadeh,
Reza. Matrix completion and low-rank SVD via fast al-
ternating least squares. arXiv:1410.2596 [stat], 2014.

Johnson, William B. and Lindenstrauss, Joram. Extensions
of Lipschitz mappings into a Hilbert space. Contempo-
rary mathematics, 26(189-206):1, 1984.

Mairal, Julien. Stochastic majorization-minimization al-
In Advances in
gorithms for large-scale optimization.
Neural Information Processing Systems, pp. 2283–2291,
2013.

Mairal, Julien. Sparse Modeling for Image and Vision Pro-
cessing. Foundations and Trends in Computer Graphics
and Vision, 8(2-3):85–283, 2014.

Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro,
Guillermo. Online learning for matrix factorization and
sparse coding. The Journal of Machine Learning Re-
search, 11:19–60, 2010.

McKeown, M. J., Makeig, S., Brown, G. G., Jung, T. P.,
Kindermann, S. S., Bell, A. J., and Sejnowski, T. J. Anal-
ysis of fMRI Data by Blind Separation into Independent
Spatial Components. Human Brain Mapping, 6(3):160–
188, 1998. ISSN 1065-9471.

Pedregosa, Fabian, Varoquaux, Gaël, Gramfort, Alexan-
dre, Michel, Vincent, Thirion, Bertrand, Grisel, Olivier,
Blondel, Mathieu, Prettenhofer, Peter, Weiss, Ron,
Dubourg, Vincent, Vanderplas, Jake, Passos, Alexan-
dre, Cournapeau, David, Brucher, Matthieu, Perrot,
Matthieu, and Duchesnay, Édouard. Scikit-learn: ma-
chine learning in Python. Journal of Machine Learning
Research, 12:2825–2830, 2011.

Pourkamali-Anaraki, Farhad, Becker, Stephen,

and
Hughes, Shannon M. Efﬁcient dictionary learning via
very sparse random projections. In Proceedings of the
IEEE International Conference on Sampling Theory and
Applications, pp. 478–482. IEEE, 2015.

Rendle, Steffen. Factorization machines. In Proceedings of
the IEEE International Conference on Data Mining, pp.
995–1000. IEEE, 2010.

Rendle, Steffen and Schmidt-Thieme, Lars.

Online-
updating regularized kernel matrix factorization models
for large-scale recommender systems.
In Proceedings
of the ACM Conference on Recommender systems, pp.
251–258. ACM, 2008.

Srebro, Nathan, Rennie, Jason, and Jaakkola, Tommi S.
Maximum-margin matrix factorization. In Advances in
Neural Information Processing Systems, pp. 1329–1336,
2004.

Szabó, Zoltán, Póczos, Barnabás, and Lorincz, András.
Online group-structured dictionary learning. In Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2865–2872. IEEE, 2011.

Dictionary Learning for Massive Matrix Factorization

Takács, Gábor, Pilászy, István, Németh, Bottyán, and Tikk,
Domonkos. Scalable collaborative ﬁltering approaches
for large recommender systems. The Journal of Machine
Learning Research, 10:623–656, 2009.

Van Essen, David C., Smith, Stephen M., Barch,
Deanna M., Behrens, Timothy E. J., Yacoub, Essa, and
Ugurbil, Kamil. The WU-Minn Human Connectome
Project: An overview. NeuroImage, 80:62–79, 2013.

Varoquaux, Gaël, Gramfort, Alexandre, Pedregosa, Fabian,
Michel, Vincent, and Thirion, Bertrand. Multi-subject
dictionary learning to segment an atlas of brain sponta-
neous activity. In Proceedings of the Information Pro-
cessing in Medical Imaging Conference, volume 22, pp.
562–573. Springer, 2011.

Varoquaux, Gaël, Schwartz, Yannick, Pinel, Philippe, and
Thirion, Bertrand. Cohort-level brain mapping: learn-
ing cognitive atoms to single out specialized regions. In
Proceedings of the Information Processing in Medical
Imaging Conference, pp. 438–449. Springer, 2013.

Yu, Hsiang-Fu, Hsieh, Cho-Jui, and Dhillon, Inderjit. Scal-
able coordinate descent approaches to parallel matrix
factorization for recommender systems. In Proceedings
of the International Conference on Data Mining, pp.
765–774. IEEE, 2012.

